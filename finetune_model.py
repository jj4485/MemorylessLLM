"""
Fine-tune a language model on the synthetic dataset for memorization experiments.

This script fine-tunes a pre-trained language model on the synthetic dataset
generated by generate_synthetic_dataset.py. It's designed to work in environments
without internet access by using locally cached models.
"""

import os
import json
import argparse
import logging
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    default_data_collator,
    set_seed
)
from datasets import load_dataset

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Set environment variables for offline mode
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

class SyntheticDataset(Dataset):
    """Dataset class for the synthetic data."""
    
    def __init__(self, data_path, tokenizer, max_length=1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Load the dataset
        logger.info(f"Loading dataset from {data_path}")
        self.examples = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.examples.append(json.loads(line)["text"])
        
        logger.info(f"Loaded {len(self.examples)} examples")
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        text = self.examples[idx]
        encodings = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        # Remove the batch dimension
        item = {key: val.squeeze(0) for key, val in encodings.items()}
        
        # For causal language modeling, the labels are the input_ids
        item["labels"] = item["input_ids"].clone()
        
        return item

def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune a language model on synthetic data")
    parser.add_argument(
        "--model_name",
        type=str,
        default="gpt2",
        help="The model to fine-tune (must be available locally)"
    )
    parser.add_argument(
        "--train_file",
        type=str,
        default="synthetic_dataset/train.jsonl",
        help="Path to the training data file"
    )
    parser.add_argument(
        "--validation_file",
        type=str,
        default="synthetic_dataset/test.jsonl",
        help="Path to the validation data file"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="finetuned_model",
        help="Directory to save the fine-tuned model"
    )
    parser.add_argument(
        "--num_train_epochs",
        type=int,
        default=3,
        help="Number of training epochs"
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=4,
        help="Batch size per device during training"
    )
    parser.add_argument(
        "--per_device_eval_batch_size",
        type=int,
        default=4,
        help="Batch size per device during evaluation"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help="Initial learning rate"
    )
    parser.add_argument(
        "--warmup_steps",
        type=int,
        default=500,
        help="Number of warmup steps"
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=0.01,
        help="Weight decay"
    )
    parser.add_argument(
        "--logging_steps",
        type=int,
        default=100,
        help="Log every X updates steps"
    )
    parser.add_argument(
        "--save_steps",
        type=int,
        default=1000,
        help="Save checkpoint every X updates steps"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed"
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=1024,
        help="Maximum sequence length"
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=8,
        help="Number of updates steps to accumulate before performing a backward/update pass"
    )
    parser.add_argument(
        "--use_datasets_library",
        action="store_true",
        help="Use the datasets library instead of custom dataset class"
    )
    return parser.parse_args()

def train_with_custom_dataset(args):
    """Train using a custom dataset implementation."""
    # Set random seed
    set_seed(args.seed)
    
    # Load tokenizer and model
    logger.info(f"Loading tokenizer and model from {args.model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(args.model_name, local_files_only=True)
        logger.info(f"Model loaded successfully")
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        raise
    
    # Create datasets
    train_dataset = SyntheticDataset(args.train_file, tokenizer, args.max_seq_length)
    eval_dataset = SyntheticDataset(args.validation_file, tokenizer, args.max_seq_length)
    
    # Define training arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        evaluation_strategy="steps",
        eval_steps=args.save_steps,
        save_total_limit=3,
        load_best_model_at_end=True,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available
        report_to="none",  # Disable reporting to wandb, etc.
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=default_data_collator,
    )
    
    # Train the model
    logger.info("Starting training")
    trainer.train()
    
    # Save the final model
    logger.info(f"Saving model to {args.output_dir}")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    return model, tokenizer

def train_with_datasets_library(args):
    """Train using the datasets library."""
    # Set random seed
    set_seed(args.seed)
    
    # Load tokenizer and model
    logger.info(f"Loading tokenizer and model from {args.model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, local_files_only=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(args.model_name, local_files_only=True)
        logger.info(f"Model loaded successfully")
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        raise
    
    # Load datasets
    data_files = {
        "train": args.train_file,
        "validation": args.validation_file
    }
    
    datasets = load_dataset(
        "json", 
        data_files=data_files, 
        field="text",
        local_files_only=True
    )
    
    # Tokenize datasets
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=args.max_seq_length,
            return_tensors="pt"
        )
    
    tokenized_datasets = datasets.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"],
        desc="Tokenizing datasets",
    )
    
    # Define training arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        evaluation_strategy="steps",
        eval_steps=args.save_steps,
        save_total_limit=3,
        load_best_model_at_end=True,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available
        report_to="none",  # Disable reporting to wandb, etc.
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=default_data_collator,
    )
    
    # Train the model
    logger.info("Starting training")
    trainer.train()
    
    # Save the final model
    logger.info(f"Saving model to {args.output_dir}")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    return model, tokenizer

def main():
    args = parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Log arguments
    logger.info(f"Training arguments: {args}")
    
    # Train model
    if args.use_datasets_library:
        model, tokenizer = train_with_datasets_library(args)
    else:
        model, tokenizer = train_with_custom_dataset(args)
    
    logger.info("Training complete!")

if __name__ == "__main__":
    main()
